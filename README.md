
# Azure Data Factory CI/CD Pipeline with Metadata-Driven Incremental Loads

## üìò Overview

This repository demonstrates a robust CI/CD pipeline setup for Azure Data Factory (ADF), leveraging metadata-driven design for full and incremental data loads. It includes environment-based deployment using Azure DevOps and dynamic pipeline configuration that supports both `datetime` and `int` watermarking strategies.

## üóÇ Project Structure

```bash
.
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_overview.png
‚îÇ   ‚îú‚îÄ‚îÄ foreach_details.png
‚îú‚îÄ‚îÄ data-factory/
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ factory/
‚îÇ   ‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îî‚îÄ‚îÄ linkedServices/
‚îú‚îÄ‚îÄ devops/
‚îÇ   ‚îú‚îÄ‚îÄ adf-build-job.yml
‚îÇ   ‚îú‚îÄ‚îÄ adf-deploy-job.yml
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ azure-pipelines.yml
‚îî‚îÄ‚îÄ README.md
```

## üöÄ CI/CD Pipeline Flow

### Triggers

```yaml
trigger:
  branches:
    include:
      - main

pr:
  branches:
    include:
      - main
```

Only the `main` branch is allowed to trigger deployments. Feature branches (e.g., `feature/*`) do not trigger the pipeline.

### Stages

#### `BUILD`

- Exports ADF ARM templates

#### `DEV`

- Deploys artifacts to **Development** environment
- Uses pipeline parameters and the `DEV` variable group

#### `PROD`

- Triggered only when changes are merged into `main`
- Uses condition:

  ```yaml
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
  ```

## üîÅ Metadata-Driven Incremental Loads

### Metadata Table

```sql
CREATE TABLE dbo.ETL_Table_Metadata (
  table_name NVARCHAR(100),
  schema_name NVARCHAR(50),
  is_incremental BIT,
  watermark_column NVARCHAR(100),
  watermark_type NVARCHAR(20),
  last_loaded_datetime DATETIME,
  last_loaded_integer INT
);
```

### Lookup Query (to fetch metadata)

```sql
SELECT 
    t.TABLE_NAME AS tableName,
    t.TABLE_SCHEMA AS schemaName,
    COALESCE(m.is_incremental, 0) AS isIncremental,
    COALESCE(m.watermark_column, '') AS watermarkColumn,
    COALESCE(m.last_loaded_datetime, '2000-01-01') AS lastLoadedDate,
    COALESCE(m.last_loaded_integer, 0) AS lastLoadedItem,
    m.watermark_type AS watermarkType
FROM INFORMATION_SCHEMA.TABLES t
LEFT JOIN dbo.ETL_Table_Metadata m
    ON t.TABLE_NAME = m.table_name
WHERE t.TABLE_SCHEMA = 'dbo' AND t.TABLE_NAME NOT IN ('ETL_Table_Metadata')
```

### Runtime SQL for Copy Activity

```expression
@{if(
  and(equals(item().isIncremental, 1), not(empty(activity('Get last watermark').output.firstRow.watermarkType))),
  if(
    equals(activity('Get last watermark').output.firstRow.watermarkType, 'int'),
    concat(
      'SELECT * FROM ', item().schemaName, '.', item().tableName,
      ' WHERE ', item().watermarkColumn, ' > ',
      activity('Get last watermark').output.firstRow.last_loaded_integer
    ),
    concat(
      'SELECT * FROM ', item().schemaName, '.', item().tableName,
      ' WHERE ', item().watermarkColumn, ' > ''',
      activity('Get last watermark').output.firstRow.last_loaded_datetime, ''''
    )
  ),
  concat('SELECT * FROM ', item().schemaName, '.', item().tableName)
)}
```

### Stored Procedure for Watermark Update

```sql
CREATE PROCEDURE [dbo].[usp_UpdateWatermark]
    @tableName NVARCHAR(255),
    @newWatermark NVARCHAR(50),
    @watermarkType NVARCHAR(20)
AS
BEGIN
    IF @watermarkType = 'datetime'
    BEGIN
        UPDATE dbo.ETL_Table_Metadata
        SET last_loaded_datetime = CONVERT(DATETIME, @newWatermark)
        WHERE table_name = @tableName;
    END
    ELSE
    BEGIN
        UPDATE dbo.ETL_Table_Metadata
        SET last_loaded_integer = CONVERT(INT, @newWatermark)
        WHERE table_name = @tableName;
    END
END;
```

## üì¶ Blob Output & File Naming Convention

- Partitioned output folder path:

```expression
@concat(
  dataset().serverName, '/',
  dataset().databaseName, '/',
  dataset().tableName, '/Year=',
  formatDateTime(utcNow(), 'yyyy'), '/Month=',
  formatDateTime(utcNow(), 'MM'), '/Day=',
  formatDateTime(utcNow(), 'dd'), '/Hour=',
  formatDateTime(utcNow(), 'HH')
)
```

- File name format:

```expression
@concat(dataset().schemaName, '_',dataset().tableName, '_', formatDateTime(utcNow(), 'yyyyMMddHHmmss'), '.parquet')
```

## üß† Parameterization Strategy

- Pipeline-level parameters feed into dataset parameters
- Dataset parameters cascade into linked service parameters
- This enables full dynamic configuration at runtime without hardcoding values

## üõ° Branch Protection Strategy

Configure via Azure DevOps under `Repos ‚Üí Branches ‚Üí Policies` on `main` branch:

- Require PR approvals
- Prevent direct pushes
- Require successful build validation before merging

## üë®‚Äçüíª Author

**Tunde Morakinyo**  
BI Developer & Azure Data Platform Engineer

---

Feel free to fork, clone, or extend this setup in your projects. Contributions welcome!
